<!doctype html>
<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>Robotic Reinforcement Learning</title>

		<link rel="stylesheet" href="css/reveal.css">
		<link rel="stylesheet" href="css/theme/black.css">

		<!-- Theme used for syntax highlighting of code -->
		<link rel="stylesheet" href="lib/css/zenburn.css">

		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>
	</head>
	<body>
		<div class="reveal">
            <header style="position: absolute;top: 50px; left: 100px; z-index:500;"><h3></h3></header>
            <footer style="position: absolute;bottom: 50px;right:150px; z-index:500;"><small></small></footer>
			<div class="slides">
                <!-- Title slide -->
				<section>
                    <h2>Motion Planning for Structured Exploration in Robotic Reinforcement Learning</h2>
                    <h3 style="text-align:center"><a href="http://cannontwo.com">Cannon Lewis</a></h3>
                </section>
                <!-- Section 1 -->
                <section data-state="header1">
                    <style>.header1 header h3:after { content: "Introduction"; }</style>
                    <div style="width:35%;float:left">
                        <img style="margin-top:0px;max-height:500px;width:auto" src="http://cannontwo.com/assets/face.jpg"/>
                    </div>
                    <div style="margin-left:5%;width:60%;float:left">
                        <h4>Who Am I?</h4>
                        <p>Senior at Rice University</p>
                        <p>Majoring in Computer Science and Mathematics</p>
                        <p>Currently applying to artificial intelligence PhD programs</p>
                        <h4>Why Am I Here?</h4>
                        <p>Took COMP 311: Functional Programming from Dr. Eric Allen</p>
                        <p>Interned at Two Sigma during Summer 2016</p>
                        <p>Currently doing research with Dr. Lydia Kavraki</p>
                    </div>
                </section>
                <!-- Section 2 -->
				<section data-state="header2">
                    <style>.header2 header h3:after { content: "Robots & Configuration Spaces"; }</style>
                    <div style="width:30%;float:left">
                        <img style="max-height:200px;width:auto" src="assets/ur5.png"/>
                        <img style="max-height:200px;width:auto" src="assets/robonaut.jpg"/>
                    </div>
                    <div style="width:25%;float:left">
                        <img style="max-height:200px;width:auto" src="assets/fetch.jpg"/> 
                        <img style="max-height:200px;width:auto" src="assets/baxter.jpg"/>
                    </div>
                    <div style="margin-left:5%;width:40%;float:left">
                        <h4>So Many Robots!</h4>
                        <p>Excitement due to recent advances in artificial intelligence</p>
                        <p>New use cases in medicine, search and rescue</p>
                        <p>New kinds of robots (e.g. drones, "safe" robotics, nanorobots)</p>
                        <p>We need to be able to plan motions for these robots in the presence of obstacles</p>
                        <p>But how do we work with these very different robots?</p>
                    </div>
                </section>
                <section data-state="header2">
                    <h4>Configuration Spaces</h4>
                    <p>In order to plan for the wide array of robots that exist, we need an abstraction, which we call the "configuration space"</p>
                    <p>Even though our robots operate in $\mathbb{R}^3$, the space of movements for a robot is more accurately represented by $\mathbb{R}^6$, or even $T^6$</p>
                    <p>Under this abstraction, we can consider certain robots as being equivalent</p>
                    <table><tr valign="center">
                        <td><img style="max-height:175px;width:auto" src="assets/ur5.png"/></td>
                        <td style="vertical-align:middle"><h5 style="display:inline-block">$\approx$</h5></td>
                        <td><img style="max-height:175px;width:auto" src="assets/sawyer.jpg"/></td>
                    </tr></table>
                </section>
                <section data-state="header2 footer1">
                    <style>.footer1 footer small:after { content: "http://www.sciencedirect.com/science/article/pii/S2095809916300443"; }</style>
                    <img style="width:80%;height:auto;" src="assets/cfgspace.jpg"/>
                </section>
                <!-- Section 3 -->
				<section data-state="header3">
                    <style>.header3 header h3:after { content: "Motion Planning"; }</style>
                    <h4>Robot Motion is Difficult</h4>
                    <p>As we have seen, the configuration space can look very different from the 3D world that we are used to</p>
                    <p>Search in this space is difficult due to
                    <ol>
                        <li>High dimensionality</li>
                        <li>Continuity</li>
                        <li>Limited computational resources</li>
                    </ol>
                    <p>These issues prevent us from using traditional search and planning methods</p>
                </section>
                <section data-state="header3 footer2">
                    <style>.footer2 footer small:after { content: "https://www.intechopen.com/books/autonomous-vehicle/application-of-sampling-based-motion-planning-algorithms-in-autonomous-vehicle-navigation"; }</style>
                    <div style="width:65%;float:left">
                        <h4>Sampling-based Motion Planning</h4>
                        <p>Instead of searching through the configuration space directly or discretizing the space, we build an implicit roadmap by sampling</p>
                        <p style="margin-bottom:5px;">Algorithms in this vein have revolutionized robot motion planning, and are now the dominant method</p>
                        <ul style="text-align:left;margin-left:40px;"><li>These algorithms are fully general - they can plan for any robot whose configuration space is connected</li></ul>
                        <p>Algorithms for SBMP such as PRM, RRT, KPIECE, and more are available out-of-the-box in my lab's <a href="http://ompl.kavrakilab.org/">OMPL</a> library</p>
                    </div>
                    <div style="width:35%;padding-top:50px;float:left;vertical-align:middle;">
                        <img style="max-height:300px;width:auto;transform:rotate(90deg)" src="assets/prm.png"/>
                    </div>
                </section>
                <section data-state="header3 footer3">
                    <style>.footer3 footer small:after { content: "http://tmkit.kavrakilab.org/"; }</style>
                    <div style="width:65%;float:left;">
                        <h4>High-level Task Planning</h4>
                        <p>In addition to making low-level motion plans, we would like to solve high-level tasks</p>
                        <p>For example, in tabletop manipulation we would like to simply tell the robot where to place dishes, instead of specifying how to move its arm for each motion</p>
                        <p><a href="http://tmkit.kavrakilab.org/">TMKit</a> is a project from our lab that performs this planning incrementally</p>
                        <p>Unfortunately, even abstract problem solving is PSPACE-complete, so exact Task-and-Motion Planning is intractable in many cases</p>
                        
                    </div>
                    <div style="width:35%;float:left;">
                        <img style="max-width:300px;height:auto;" src="assets/tmkit.jpg"/>
                    </div>  
                </section>
                <!-- Section 4 -->
				<section data-state="header4">
                    <style>.header4 header h3:after { content: "Reinforcement Learning"; }</style>
                    <div style="width:45%;float:left">
                        <h4>Theoretical Basics</h4>
                        <p>RL is a form of machine learning in which a learning agent is provided with a real-valued reward function in response to its actions</p>
                        <p>RL stems from research into Markov Decision Processes (MDPs)</p>
                        <p>Goal is to build an agent which can learn a maximizing policy $\pi(s,a)$ for an arbitrary MDP that the agent is interacting with</p>
                    </div>
                    <div style="width:45%;float:left;margin-left:10%">
                        <h4>What is an MDP?</h4>
                        <ul>
                            <li>A set of states $S$, set of actions $A$.</li>
                            <li>Stochastic transition function $P_a(s, s')$</li>
                            <li>Reward function $R(s, a, s')$</li>
                        </ul>
                        <h4 style="margin-top:10px">Value Functions</h4>
                        <p>For elementary solution methods we care about:</p>
                        <ul>
                            <li>$V^\pi(s) = \mathbf{E}_\pi\{R_t | s_t = s\} = \mathbf{E}_\pi\left\{\sum_{k=0}^{\infty}\gamma^kr_{t+k+1} | s_t = s\right\}$</li>
                            <li>$Q^\pi(s, a) = \mathbf{E}_\pi\{R_t | s_t = s, a_t = a\}$<br/> &emsp; &emsp;&emsp;&emsp;$= \mathbf{E}_\pi\left\{\sum_{k=0}^{\infty}\gamma^kr_{t+k+1} | s_t = s, a_t = a\right\}$</li>
                        </ul>
                    </div>
                </section>
                <section data-state="header4 footer4">
                    <style>.footer4 footer small:after { content: "http://cs.stanford.edu/people/karpathy/reinforcejs/"; }</style>
                    <h4>Gridworld</h4>
                    <a href="http://cs.stanford.edu/people/karpathy/reinforcejs/"><img src="assets/gridworld.jpeg"/></a>
                </section>
                <section data-state="header4">
                    <h4>Simple Control</h4>
                    <img src="https://thumbs.gfycat.com/CluelessLittleAttwatersprairiechicken-size_restricted.gif"/>
                </section>
                <section data-state="header4">
                    <h4>RL Toolchain</h4>
                    <p>Due to recent high-profile reinforcement learning results, many open-source RL libraries are available</p>
                    <p>Prominent among these is <a href="https://github.com/openai/gym">OpenAI Gym</a>. Inexplicably, this project seems to have been abandoned recently</p>
                    <p>OpenAI also created <a href="https://github.com/openai/universe">Universe</a> and <a href="https://github.com/openai/roboschool">RoboSchool</a>, but these are similarly poorly maintained</p>
                    <p><a href="https://githubcom/SerpentAI/SerpentAI">Serpent AI</a> is a high-quality recent addition to this landscape, originating from frustration with OpenAI Universe</p>
                </section>
                <!-- Section 5 -->
				<section data-state="header5 footer5">
                    <style>.header5 header h3:after { content: "Deep Reinforcement Learning"; }</style>
                    <style>.footer5 footer small:after { content: "http://karpathy.github.io/2016/05/31/rl/"; }</style>
                    <div style="width:65%;float:left;">
                        <h4>Limitations of Naive RL</h4>
                        <p>Exact solution is prohibitively expensive in both time and space for general MDPs</p>
                        <p>Furthermore, an enumerative approach to value function calculation is incompatible with continuous state or action spaces</p>
                        <h4 style="margin-top:10px">Deep Learning</h4>
                        <p>To overcome these difficulties, we introduce function approximation, typically through neural networks</p>
                        <p>We can parameterize either the value functions or the policy as a neural network, then use typical gradient descent techniques to train our RL agent</p>
                    </div>
                    <div style="width:35%;float:left;">
                        <img style="margin-top:100px;margin-left:50px" src="assets/deeprl.png"/>
                    </div>
                </section>
                <section data-state="header5 footer6">
                    <style>.footer6 footer small:after { content: "https://deepmind.com/research/dqn/"; }</style>
                    <div style="width:65%;float:left">
                        <h4>Q-Function Learning: DQN</h4>
                        <p>The Q-function encodes how valuable actions in each state are</p>
                        <p>If we use a neural network to represent our $Q^\pi(s,a)$, we can do supervised learning using rewards we get from the MDP</p>
                        <p>This leads naturally to the Deep Q-Network algorithm, which was published by Google Deepmind in 2015</p>
                        <p>However, DQN is limited to discrete action spaces</p>
                    </div>
                    <div style="width:35%;float:left">
                        <img style="margin-left:50px" src="assets/dqn.jpg"/>
                    </div>
                </section>
                <section data-state="header5 footer7">
                    <style>.footer7 footer small:after { content: "https://arxiv.org/abs/1509.02971"; }</style>
                    <div style="width:65%;float:left">
                        <h4>Continuous Action Spaces: DDPG</h4>
                        <p>To deal with continuous action spaces, we need to represent our policy by a neural network</p>
                        <p>The Deep Deterministic Policy Gradients algorithms stems from this observation and naive "actor-critic" methods</p>
                        <p>DDPG is also a recent (2016) result out of Deepmind</p>
                        <p>Now, we are finally (theoretically) capable of tackling robotic control tasks</p>
                    </div>
                    <div style="width:35%;float:left">
                        <img style="margin-left:50px;" src="assets/actor-critic.png"/>
                    </div>
                </section>
                <!-- Section 6 -->
				<section data-state="header6 footer8">
                    <style>.header6 header h3:after { content: "Robotic Reinforcement Learning"; }</style>
                    <style>.footer8 footer small:after { content: "http://www.ias.tu-darmstadt.de/uploads/Publications/Kober_IJRR_2013.pdf"; }</style>
                    <div style="width:50%;float:left">
                        <h4>Tractability Through Simplification</h4>
                        <p>Most research previous to 2015 in robotic reinforcement learning (RRL) simplified the task by modifying the action space</p>
                        <ul style="text-align:left;margin-left:40px;">
                            <li>Jans Kober's work into dynamical system motor primitives</li>
                            <li>Physics model-based approaches such as CMU's inverted helicopter research</li>
                            <li>Myriad discretizations of the state and action spaces</li>
                        </ul>
                    </div>
                    <div style="width:50%;float:left">
                        <iframe style="margin-left:40px" width="560" height="315" src="https://www.youtube.com/embed/rbSFlLtnMFM" frameborder="0" allowfullscreen></iframe>
                    </div>
                </section>
                <section data-state="header6 footer9">
                    <style>.footer9 footer small:after { content: "https://arxiv.org/pdf/1707.08817.pdf"; }</style>
                    <div style="width:65%;float:left">
                        <h4>Recent UC Berkeley, Deepmind Work</h4>
                        <p>Groups at UC Berkeley and Google Deepmind have demonstrated success in using Deep RL for robotic tasks</p>
                        <p>However, this research still tends to simplify the state and action space involved by using a subset of the robot's joints</p>
                        <p>Even taking the reduced space, RRL can be intractable without human demonstration for initialization</p>
                    </div>
                    <div style="width:35%;float:left">
                        <img src="assets/deepmind_robot.png"/>
                    </div>
                </section>
                <section data-state="header6">
                    <h4>Current Problems</h4>
                    <ol>
                        <li>For more complex tasks, we need to be able to learn robot policies involving all joints in the robot</p>
                        <li>For difficult or dangerous tasks, it may not be feasible to provide human expert demonstration</p>
                        <li>Furthermore, the precise difficulty of RRL is not fully known right now</p>
                        <li>Experimental setups tend to be ad hoc, involve unreported engineering effort, and unreproducible</p>
                    </ol>
                </section>
                <!-- Section 7 -->
				<section data-state="header7">
                    <style>.header7 header h3:after { content: "Motion Planning as Structured Exploration"; }</style>
                    <h4>Exploration</h4>
                    <p>So far, we have avoided discussing exploration in RL</p>
                    <p>The "exploration-exploitation" tradeoff is a big deal in theoretical results for RL</p>
                    <p>In practice, most algorithms simply use some form of probabilistic noise as exploration</p>
                    <p>For high-dimensional spaces, such as RRL, this is likely not sufficient</p>
                </section>
                <section data-state="header7">
                    <div style="width:65%;float:left">
                        <h4>Motion Planning as Exploration</h4>
                        <p>Recall that sampling-based motion planning allows us to generate motion plans for arbitrary robots</p>
                        <p>These motion plans are effectively structured, sustained exploration from the perspective of an RRL learner</p>
                        <p>If we can correctly hook motion planning for exploration into existing deep RL methods, it could replace human demonstration</p>
                        <p>Furthermore, this allows RRL to be environment-agnostic, a goal of reinforcement learning</p>
                    </div>
                    <div style="width:35%;float:left">
                        <table style="margin-left:50px">
                            <tr><td style="border:none"><img src="assets/moveit_logo.jpg"/></td></tr>
                            <tr><td style="border:none;text-align:center"><h2>+</h2></td></tr>
                            <tr><td><img src="assets/gym.png"/></td></tr>
                        </table>
                    </div>
                </section>
                <section data-state="header7">
                    <!-- Show gif of Fetch -->
                    <h4>Learning on a Fetch Robot</h4>
                    <img src=""/>
                </section>
                <section data-state="header7">
                    <!-- Show gif of UR5 -->
                    <h4>Learning on a UR5 Robot</h4>
                    <img src=""/>
                </section>
                <section data-state="header7">
                    <!-- Show grid of failed experiments -->
                    <h4>Hyperparameter Search is Difficult</h4>
                    <img src=""/>
                </section>
                <!-- Section 8 -->
				<section data-state="header8">
                    <style>.header8 header h3:after { content: "Experimentation"; }</style>
                    <!-- <p>Deepmind's unpublished architecture, Google scale, etc.</p> -->
                    <!-- <p>Using AWS, ROS, OpenAI Gym, and Python to create an (eventually) open-source alternative.</p> -->
                    <h4>RRL Experimentation is Difficult</h4>
                    <p>As we have seen, many technical hurdles exist for the budding RRL researcher</p>
                    <ul style="text-align:left;margin-left:-200px;">
                        <li>Many tools are not open-source or not well-maintained</li>
                        <li>Myriad hyperparameters must be tuned</li>
                        <li>It is not well known where the difficult problems lie</li>
                        <li>Most robotics researchers do not have Google-scale compute resources</li>
                    </ul>
                    <br/>
                    <p>I intend to publish a paper soon dealing with the complexity of RRL for different numbers of joints</p>
                    <p>Much, much more research is left in this area</p>
                </section>
                <section data-state="header8">
                        <h4>Building an Open-Source Stack</h4>
                        <p>I have used entirely open source tools</p>
                        <p>Built my own versions of proprietary tools for experiment running</p>
                        <p>Once I have published my research, I intend to open source my experiment code as well</p>
                        <p>Continued open-source collaboration is necessary to progress in CS research</p>
                        <table style="margin-left:50px">
                            <tr>
                            <td style="height:100px;width:auto;border:none"><img src="assets/gym.png"/></td>
                            <td style="height:100px;width:auto;border:none"><img src="assets/ros.png"/></td>
                            <td style="height:100px;width:auto;border:none"><img src="assets/moveit_logo.jpg"/></td>
                            <td style="height:100px;width:auto;border:none"><img src="assets/docker.png"/></td>
                            </tr>
                        </table>
                </section>
                <section data-state="header8 footer10">
                    <style>.footer10 footer small:after { content: "http://www.humphreysheil.com/blog/gorila-google-reinforcement-learning-architecture"; }</style>
                    <div style="width:60%;float:left">
                        <h4>A Case Against Proprietary Research Systems</h4>
                        <p>In this talk, I have mentioned Google Deepmind several times</p>
                        <p>One of their main research advantages is a distributed RL system that they call "Gorila"</p>
                        <p>Deepmind has not released this system design to the public</p>
                        <p>Proprietary advantages like this inhibit the progress of research in RL</p>
                        <p>I built a similar small distributed system using Docker and Amazon Web Services for my research</p>
                    </div>
                    <div style="width:40%;float:left">
                        <img style="margin-left:50px" src="assets/gorila.png"/>
                    </div>
                </section>
                <section>
                    <h1>Thank You</h1>
                </section>
			</div>
		</div>

		<script src="lib/js/head.min.js"></script>
		<script src="js/reveal.js"></script>

		<script>
			// More info about config & dependencies:
			// - https://github.com/hakimel/reveal.js#configuration
			// - https://github.com/hakimel/reveal.js#dependencies
			Reveal.initialize({
                slideNumber: true,
                history: true,
                math: {
					// mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js',
					config: 'TeX-AMS_HTML-full'
				},
				dependencies: [
					{ src: 'plugin/markdown/marked.js' },
					{ src: 'plugin/markdown/markdown.js' },
					{ src: 'plugin/notes/notes.js', async: true },
					{ src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
                    { src: 'plugin/math/math.js', async: true }
				]
			});
		</script>
	</body>
</html>
